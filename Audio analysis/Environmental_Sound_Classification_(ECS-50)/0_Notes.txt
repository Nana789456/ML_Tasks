
--------------------Classical machine learning----------------------
## KNN - 0.3465 

## LogisticRegression - 0.4594 

## SVM - 0.4615 

## RandomForestClassifier - 0.48 

## CatBoost - 0.495

--------------------Deep Machine Learning----------------------
- Максимальная точность в 0.5125 была достигнута на датасете Samples_load_42 (категориальные данные) с применением архитектуры полносвязной сети.
- Максимальная точность на связанных данных (датасет Chr_zcr_sc_features) была всего 0.37.

- Для повышения точности предсказания на сложных связанных данных (где как раз и сильно глубокое машинное обучение) можно попробовать следующие способы:
        <> Добавить фичей в датасет Chr_zcr_sc_features, например 20 фильтров mfcc можно обработать по аналогии с частотами цветности (функция get_features_stft_zcr_sc) взяв std или mean между фильтрами. Соответственно после добавления новой фичи в датасет можно добавить новую ветвь в сложную архитектуру.
        <> В сверточную архитектуру можно добавить пробросы с более ранних слоёв (как в U-net)
        <> Использовать полную Спектрограмму (раздел Визуализация спектрограммы, где размерность 1025, 431) в качестве двумерного массива для обработки Conv2D сетями.
        <> В одной из статей:
        https://github.com/bkasvenkatesh/Classifying-Environmental-Sounds-with-Image-Networks/blob/master/Final_thesis_report.pdf
        спектрограмма, MFCC и CRP были преобразованы в изображения и подавались на предобученные НС, такие как AlexNet и GoogLeNet. Данная тактика привела к точности в 73% на ESC-50 (обучающие данные были аугментированы). Поэтому можно попробовать и данный вариант, однако с базой придется повозиться.
        <> Можно использовать другие рекуррентые слои, например LSTM
        <> Поробовать применить feature embeddings (для категорильных данных, т.е. датасетов типа 37_features и 42_features)
        <> Произвести дополнительную аугментацию данных